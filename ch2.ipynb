{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDkYFwM6N7z+4dVqW81TBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dohy-Lee/NLP_by_Pytorch/blob/main/ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sec8PlsJlYc"
      },
      "outputs": [],
      "source": [
        "# 샘플(데이터 포인트) : 메타데이터가 붙은 텍스트\n",
        "# 데이터셋 : 샘플의 모음인 말뭉치\n",
        "# 토큰화 : 텍스트를 토큰으로 나누는 과정. 보통 구두점이나 공백 문자로 구분.\n",
        "# 한국어와 튀르키예어 같은 교착어(어근과 접사가 단어의 기능을 결정하는 언어)는 공백 문자와 구두점으로 나누는 것으론 부족함.\n",
        "# 텍스트를 바이트 스트림으로 신경망에 표현하여 이와 같은 문제를 완전히 피해 갈 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 처리 분야에 널리 쓰이는 패키지 NLTK을 통해 텍스트 토큰화"
      ],
      "metadata": {
        "id": "HnhXAQ-ULQ-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"Mary, don't slap the green witch.\"\n",
        "print([str(token) for token in nlp(text.lower())])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BET0Gz3LWMS",
        "outputId": "ce7ca9e3-3a5f-46d6-c1cc-888ed89e0c1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)\"\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNI_vmksLf6y",
        "outputId": "3e04af05-5561-4204-c685-f04e98725344"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 타입 : 말뭉치에 등장하는 고유한 토큰. 말뭉치에 있는 모든 타입의 집합이 어휘 사전 또는 어휘임\n",
        "# 단어는 내용어와 불용어로 구분됨. 관사와 전치사 같은 불용어는 대부분 내용어를 보충하는 문법적인 용도로 사용"
      ],
      "metadata": {
        "id": "FP28hBp4PEue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 유니그램, 바이그램, 트라이그램, ⋯, n-그램\n",
        "* n-그램 : 텍스트에 있는 고정 길이(n)의 연속된 토큰 시퀀스\n",
        "* 바이그램 : 토큰 두 개\n",
        "* 유니그램 : 토큰 한개"
      ],
      "metadata": {
        "id": "40BgB4vKPxoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트에서 n-그램 만들기\n",
        "def n_grams(text,n):\n",
        "  '''\n",
        "  takes tokens or text, returns a list of n-grams\n",
        "  '''\n",
        "  return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "\n",
        "cleand = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
        "print(n_grams(cleand,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scA1h2z3P9Sg",
        "outputId": "60ca75bc-ea20-4b5b-9dc9-3c87e30224fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 표제어와 어간\n",
        "* 표제어 : 단어의 기본형, 즉 사전에 등재된 단어\n",
        "* 표제어 추출 : 토큰을 표제어로 바꾸어 벡터 표현의 차원을 줄이는 방법\n",
        "* 어간 추출 : 표제어 추출 대신 사용하는 축소 기법. 수동으로 만든 규칙을 사용해 단어의 끝을 잘라 어간이라는 공통 형태로 축소\n",
        "\n",
        " ex) geese를 표제어 추출할 경우 goose, 어간 추출할 경우 gees가 생성"
      ],
      "metadata": {
        "id": "eb9TLLUORLVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 단어를 표제어로 변환\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"he was running late\")\n",
        "for token in doc:\n",
        "  print('{} --> {}'.format(token, token.lemma_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mi1ahe1QoX2",
        "outputId": "1aae927b-36d1-4dd0-901b-44ed81ad9eec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he --> he\n",
            "was --> be\n",
            "running --> run\n",
            "late --> late\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문장과 문서 분류하기\n",
        "* 문서를 분류하는 작업은 NLP 분야의 초기 애플리케이션 중 하나\n",
        "* TF와 TF-IDF 표현이 문서나 문장 같은 긴 텍스트 뭉치를 분류하는 데 유용"
      ],
      "metadata": {
        "id": "rKdvXAozSa2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 분류하기: 품사 태깅\n"
      ],
      "metadata": {
        "id": "bevIhxrsSrG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 태깅\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Mary slapped the green witch.\")\n",
        "for token in doc:\n",
        "  print('{} - {}'.format(token, token.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A3hjT1JRqgo",
        "outputId": "16987799-9aa6-48b9-f69a-a45604b4a189"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary - PROPN\n",
            "slapped - VERB\n",
            "the - DET\n",
            "green - ADJ\n",
            "witch - NOUN\n",
            ". - PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 청크 나누기와 개체명 인식\n",
        "* 청크나누기(부분 구문 분석) : 연속된 여러 토큰으로 구분되는 텍스트 구에 레이블을 할당\n",
        " * 명사, 동사, 형용사 같은 문법 요소로 구성된 고차원의 단위를 유도해 내는 것이 목표\n",
        " * 부분 구문 분석 모델 훈련에 사용할 데이터가 없다면 품사 태깅에 정규식을 적용해 부분 구문 분석을 근사할 수 있음\n",
        "* 개채명 : 사람, 장소, 회사, 약 이름 등 실제 세상의 개념을 의미하는 문자열"
      ],
      "metadata": {
        "id": "SaCDuZtNTnzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'Mary slapped the green witch.')\n",
        "for chunk in doc.noun_chunks:\n",
        "  print('{} - {}'.format(chunk, chunk.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FPHii1yTAha",
        "outputId": "b3bf36ac-102d-4d42-d72b-5a4067a3ce2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary - NP\n",
            "the green witch - NP\n"
          ]
        }
      ]
    }
  ]
}