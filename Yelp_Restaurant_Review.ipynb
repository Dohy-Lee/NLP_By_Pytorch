{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171fd363",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fdcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #정규 표현 처리를 위한 모듈\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def70e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_train_dataset_csv=\"yelp/raw_train.csv\",\n",
    "    raw_test_dataset_csv=\"yelp/raw_test.csv\",\n",
    "    proportion_subset_of_train=0.1,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"yelp/reviews_with_splits_lite.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80066300",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=['rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf5fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 클래스 비율이 동일하도록 만듭니다\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in train_reviews.iterrows(): #Pandas의 iterrows : (인덱스, row값) 반환\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "#     if _ == 1 :#위 코드가 어떻게 실행되는지 확인용\n",
    "#         print(row.rating, by_rating[row.rating]) \n",
    "#         print(row.to_dict())\n",
    "#         print(by_rating)\n",
    "review_subset = []\n",
    "#print(len(by_rating)) : 결과값 2 → {1: 1점짜리 리뷰들, 2: 2점짜리 리뷰들}\n",
    "for _, item_list in sorted(by_rating.items()): #키(_)와 벨류(item_list)가 iterator, 총 2번 실행\n",
    "                                               #벨류인 item_list에는 1점짜리 리뷰들, 2점짜리 리뷰들이 있음.\n",
    "    n_total = len(item_list)\n",
    "    n_subset = int(args.proportion_subset_of_train * n_total) # proportion_subset_of_train를 곱해주지 않으면, 전체데이터 560,000개 저장\n",
    "    review_subset.extend(item_list[:n_subset]) #각 1점짜리 리뷰들과 2점짜리 리뷰들의 10%만 저장\n",
    "#     if _ == 1 :#위 코드가 어떻게 실행되는지 확인용\n",
    "#         print('n_total',n_total) \n",
    "#         print('n_subset', n_subset)\n",
    "#         print(review_subset)\n",
    "#     print(_)\n",
    "#print(len(review_subset))\n",
    "review_subset = pd.DataFrame(review_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575b86a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       rating                                             review\n",
      "0           1  Unfortunately, the frustration of being Dr. Go...\n",
      "1           1  I don't know what Dr. Goldberg was like before...\n",
      "2           1  I'm writing this review to give you a heads up...\n",
      "3           1  Wing sauce is like water. Pretty much a lot of...\n",
      "4           1  Owning a driving range inside the city limits ...\n",
      "...       ...                                                ...\n",
      "27995       1  I'd hate to break the chain of good reviews bu...\n",
      "27996       1  Not only does this place smells, but also I we...\n",
      "27997       1  guy behind counter spitting out tobacco juice ...\n",
      "27998       1  Owners are total asswipes. [David & Matthew Mc...\n",
      "27999       1  How can this store be in any way related to th...\n",
      "\n",
      "[28000 rows x 2 columns]\n",
      "       rating                                             review\n",
      "28000       2  Been going to Dr. Goldberg for over 10 years. ...\n",
      "28001       2  All the food is great here. But the best thing...\n",
      "28002       2  Before I finally made it over to this range I ...\n",
      "28003       2  I drove by yesterday to get a sneak peak.  It ...\n",
      "28004       2  Wonderful reuben.  Map shown on Yelp page is i...\n",
      "...       ...                                                ...\n",
      "55995       2  I am not really an arts and crafts kind of guy...\n",
      "55996       2  i absolutely love michael's! i used to scrapbo...\n",
      "55997       2  The fact that I can generally get whatever I w...\n",
      "55998       2  I've been frequenting Michaels lately because ...\n",
      "55999       2  Narrow aisles and not much for home decor like...\n",
      "\n",
      "[28000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(review_subset[:28000])\n",
    "print(review_subset[28000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c1822d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wing sauce is like water. Pretty much a lot of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Owning a driving range inside the city limits ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       1  I don't know what Dr. Goldberg was like before...\n",
       "2       1  I'm writing this review to give you a heads up...\n",
       "3       1  Wing sauce is like water. Pretty much a lot of...\n",
       "4       1  Owning a driving range inside the city limits ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e28d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    280000\n",
       "2    280000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f34ef643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유 클래스\n",
    "set(review_subset.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768d094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 검증, 테스트를 만들기 위해 별점을 기준으로 나누기\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in review_subset.iterrows(): #위에서 만든 56,000개의 데이터\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "\n",
    "# 분할 데이터\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "\n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    n_total = len(item_list)\n",
    "    n_train = int(args.train_proportion * n_total) # 1점 짜리 리뷰들, 2점짜리 리뷰들 각각의 28,000개 중 70%\n",
    "    n_val = int(args.val_proportion * n_total)# 1점 짜리 리뷰들, 2점짜리 리뷰들 각각의 28,000개 중 15%\n",
    "    n_test = int(args.test_proportion * n_total)# 1점 짜리 리뷰들, 2점짜리 리뷰들 각각의 28,000개 중 15%\n",
    "    \n",
    "    # 데이터 포인터에 분할 속성(훈련데이터인지, 검증 데이터인지, 테스트 데이터인지)을 추가\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    \n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "        \n",
    "    for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "        item['split'] = 'test'\n",
    "    # 최종 리스트에 추가\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa2cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 데이터→데이터 프레임 변환\n",
    "final_reviews = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa64421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    39200\n",
       "val       8400\n",
       "test      8400\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.split.value_counts() #훈련 데이터, 검증 데이터, 테스트 데이터 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91568254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "# re.sub（정규 표현식, 치환 문자, 대상 문자열）: 정규 표현식:검색 패턴을 지정 \n",
    "#                                                치환 문자:변경하고 싶은 문자\n",
    "#                                                대상 문자열:검색 대상이 되는 문자열\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text) # 마침표가 있으면, 해당 문자열 앞뒤로 공백이 있는 문자열로 치환\n",
    "                                              # \\1은 정규식의 그룹 중 첫 번째 그룹\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text) #문자와 마침표가 아니라면, 공백 생성\n",
    "    return text\n",
    "    \n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4065bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews['rating'] = final_reviews.rating.apply({1: 'negative', 2: 'positive'}.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d3f14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  terrible place to work for i just heard a stor...  train\n",
       "1  negative   hours , minutes total time for an extremely s...  train\n",
       "2  negative  my less than stellar review is for service . w...  train\n",
       "3  negative  i m granting one star because there s no way t...  train\n",
       "4  negative  the food here is mediocre at best . i went aft...  train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664d24",
   "metadata": {},
   "source": [
    "# 리뷰 감성 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bd1929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83641428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653541a6",
   "metadata": {},
   "source": [
    "파이토치는 Dataset 클래스로 데이터셋을 추상화  \n",
    "Dataset 클래스는 추상화된 iterator  \n",
    "파이토치에서 새로운 데이터셋을 사용할 때는 먼저 Dataset 클래스를 상속하여 \\__ getitem \\__ 과  \\__ len \\__ 메소드 구현해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cced88d",
   "metadata": {},
   "source": [
    "# 데이터셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "786aae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset): # 1.데이터셋이 최소한으로 정제 2. 데이터 셋이 3개로 나뉨\n",
    "                              # 3.공백을 기준으로 나눠서 토큰 리스트를 얻을 수 있음 4. 훈련, 검증, 테스트 중 어디에 속해 있는지 표시 \n",
    "                              # 위 4가지가 모두 만족한다는 가정 하.\n",
    "    def __init__(self, review_df, vectorizer):# review_df (pandas.DataFrame): 데이터셋\n",
    "                                              # vectorizer (ReviewVectorizer): ReviewVectorizer 객체\n",
    "\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), # 데이터, 데이터 크기 순으로 튜플형태로 저장\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod # 클래스 메서드는 정적 메서드처럼 인스턴스 없이 호출할 수 있다는 점은 같지만\n",
    "                 #        메서드 안에서 클래스 속성, 클래스 메서드에 접근해야 할 때 사용\n",
    "                 # 특히 cls를 사용하면 메서드 안에서 현재 클래스의 인스턴스를 만들 수 있음\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv): # 데이터셋을 로드하고 새로운 ReviewVectorizer 객체 만듦\n",
    "                                                           # review_csv (str): 데이터셋의 위치\n",
    "                                                           # 반환값 : ReviewDataset의 인스턴스\n",
    "\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath): # 데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듦\n",
    "                                                                                # 캐시된 ReviewVectorizer 객체를 재사용할 때 사용\n",
    "                                                                                # review_csv (str): 데이터셋의 위치\n",
    "                                                                                # vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "                                                                                # 반환값 : ReviewDataset의 인스턴스\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath): # 파일에서 ReviewVectorizer 객체를 로드하는 정적 메서드\n",
    "                                                   # vectorizer_filepath (str): 직렬화된 ReviewVectorizer 객체의 위치\n",
    "                                                   # 반환값: ReviewVectorizer의 인스턴스\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath): # ReviewVectorizer 객체를 json 형태로 디스크에 저장\n",
    "                                                    # vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self): # 벡터 변환 객체를 반환\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"): # 데이터프레임에 있는 열을 사용해 분할 세트를 선택\n",
    "                                        #  split (str): \"train\", \"val\", \"test\" 중 하나\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index): # 파이토치 데이터셋의 주요 진입 메서드\n",
    "                                  # index (int): 데이터 포인트의 인덱스\n",
    "                                  #반환값 : 데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
    "            \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size): # 배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환\n",
    "                                           # batch_size (int)\n",
    "                                           # 반환값 : 배치 개수\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e4e3a",
   "metadata": {},
   "source": [
    "## 텍스트 → 벡터의 미니배치\n",
    "1. 토큰을 정수로 매핑  \n",
    "토큰과 정수 사이를 일대일 매핑하는 방법이 표준 → 반대로도 매핑이 가능하기 때문임\n",
    "2. 입력 데이터 포인트의 토큰을 순회하면서 각 토큰을 정수로 바꾸기. 이 반복 과정의 결과는 벡터  \n",
    "이 벡터가 다른 데이터 포인트에서 만든 벡터와 합쳐지므로 Vectorizer에서 만든 벡터는 항상 길이가 같아야함\n",
    "3. 벡터로 변환한 데이터 포인트 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7d0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object): # 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"): # token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "                                                                             # add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
    "                                                                             # unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self): # 직렬화할 수 있는 딕셔너리를 반환\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents): # 직렬화된 딕셔너리에서 Vocabulary 객체를 만듦\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token): # 토큰을 기반으로 매핑 딕셔너리를 업데이트\n",
    "                                # token (str) : Vocabulary에 추가할 토큰\n",
    "                                #반환값 : index (int): 토큰에 상응하는 정수\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens): # 토큰 리스트를 Vocabulary에 추가\n",
    "                                # tokens (list): 문자열 토큰 리스트     \n",
    "                                #indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token): #토큰에 대응하는 인덱스를 추출, 토큰이 없으면 UNK 인덱스를 반환\n",
    "                                   #token (str): 찾을 토큰 \n",
    "                                   #index (int): 토큰에 해당하는 인덱스\n",
    "                                   #UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해) `unk_index`가 0보다 커야 합니다.\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index): #인덱스에 해당하는 토큰을 반환\n",
    "                                    #index (int): 찾을 인덱스\n",
    "                                    #token (str): 인텍스에 해당하는 토큰\n",
    "                                    #KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Vocabulary에 인덱스(%d)가 없습니다.\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544fc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object): # 리뷰 텍스트를 수치 벡터로 변환하는 클래스. 어휘 사전을 생성하고 관리\n",
    "\n",
    "    def __init__(self, review_vocab, rating_vocab): # review_vocab (Vocabulary): 단어를 정수에 매핑하는 Vocabulary\n",
    "                                                    # rating_vocab (Vocabulary): 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review): # 리뷰에 대한 원-핫 벡터 생성\n",
    "                                 # review (str): 리뷰\n",
    "                                 # one_hot (np.ndarray): 원-핫 벡터\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25): # 데이터셋 데이터프레임에서 Vectorizer 객체를 만듦\n",
    "                                                   # review_df (pandas.DataFrame): 리뷰 데이터셋\n",
    "                                                   # cutoff (int): 빈도 기반 필터링 설정값\n",
    "                                                   # ReviewVectorizer 객체\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # 점수를 추가\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        # count > cutoff인 단어를 추가\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents): # 직렬화된 딕셔너리에서 ReviewVectorizer 객체를 만듦\n",
    "                                          # contents (dict): 직렬화된 딕셔너리\n",
    "                                          # ReviewVectorizer 클래스 객체\n",
    "\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "\n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self): # 캐싱을 위해 직렬화된 딕셔너리를 만듦\n",
    "                               # contents (dict): 직렬화된 딕셔너리\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea8e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, # 파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "                     drop_last=True, device=\"cpu\"):     # 각 텐서를 지정된 장치로 이동\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4049ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module): # 간단한 퍼셉트론 기반 분류기\n",
    "    def __init__(self, num_features): # num_features (int): 입력 특성 벡트의 크기\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False): # 분류기의 정방향 계산\n",
    "                                                   # x_in (torch.Tensor): 입력 데이터 텐서, x_in.shape는 (batch, num_features)\n",
    "                                                   # apply_sigmoid (bool): 시그모이드 활성화 함수를 위한 플래그\n",
    "                                                   # 크로스-엔트로피 손실을 사용하려면 False로 지정\n",
    "                                                   # 반환값은 결과 텐서. tensor.shape은 (batch,)입니다.\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f251ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ff45a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\tmodel_storage/ch3/yelp/vectorizer.json\n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='yelp/reviews_with_splits_lite.csv',\n",
    "    # review_csv='data/yelp/reviews_with_splits_full.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # 모델 하이퍼파라미터 없음\n",
    "    # 훈련 하이퍼파라미터\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc9a9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state): # 훈련 상태를 업데이트(조기 종료: 과대 적합 방지, 모델 체크포인트: 더 나은 모델을 저장)\n",
    "                                                  # args: 메인 매개변수\n",
    "                                                  # model: 훈련할 모델\n",
    "                                                  # train_state: 훈련 상태를 담은 딕셔너리\n",
    "                                                  # 반환값으로 새로운 훈련 상태\n",
    "    # 적어도 한 번 모델을 저장\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7644e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋을 로드하고 Vectorizer를 만듭니다\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # 체크포인트에서 훈련을 다시 시작\n",
    "    print(\"데이터셋과 Vectorizer를 로드합니다\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv,\n",
    "                                                            args.vectorizer_file)\n",
    "else:\n",
    "    print(\"데이터셋을 로드하고 Vectorizer를 만듭니다\")\n",
    "    # 데이터셋과 Vectorizer 만들기\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6016da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력 계산\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 3. 손실 계산\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            \n",
    "            # 정확도를 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                  acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 2. 손실을 계산\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "147c9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력 계산\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c998d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.214\n",
      "테스트 정확도: 91.81\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5e3e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77cac6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5): # 리뷰 점수 예측하기\n",
    "                                                                            # review (str): 리뷰 텍스트\n",
    "                                                                            # classifier (ReviewClassifier): 훈련된 모델\n",
    "                                                                            # vectorizer (ReviewVectorizer): Vectorizer 객체\n",
    "                                                                            # decision_threshold (float): 클래스를 나눌 결정 경계\n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9efa3cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d640c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7326])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ed1345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "delicious\n",
      "fantastic\n",
      "pleasantly\n",
      "amazing\n",
      "great\n",
      "vegas\n",
      "excellent\n",
      "yum\n",
      "perfect\n",
      "awesome\n",
      "ngreat\n",
      "yummy\n",
      "love\n",
      "bomb\n",
      "solid\n",
      "pleased\n",
      "wonderful\n",
      "chinatown\n",
      "notch\n",
      "deliciousness\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "부정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "worst\n",
      "mediocre\n",
      "bland\n",
      "horrible\n",
      "meh\n",
      "awful\n",
      "rude\n",
      "terrible\n",
      "tasteless\n",
      "overpriced\n",
      "disgusting\n",
      "unacceptable\n",
      "poorly\n",
      "slowest\n",
      "unfriendly\n",
      "nmaybe\n",
      "disappointing\n",
      "disappointment\n",
      "downhill\n",
      "underwhelmed\n"
     ]
    }
   ],
   "source": [
    "# 가중치 정렬\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# 긍정적인 상위 20개 단어\n",
    "print(\"긍정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# 부정적인 상위 20개 단어\n",
    "print(\"부정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
